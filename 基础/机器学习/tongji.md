## 概论
**1.目标**  
考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析，同时也要考虑尽可能提高学习效率  
**2.方法**  
从给定的、有限的、用于学习的训练数据（tarining data）集合出发，假设数据是独立同分布的；假设要学习的模型属于某个函数的集合，称为假设空间（hypothesis space）;应用某个评价准则（evaluation criterion），从假设空间中选取一个最优的模型，使它对于已知训练数据及测试数据（test data）在给定的评价准则下有最优的预测；最优模型由算法实现。  
统计学习方法三要素：模型的假设空间、模型选择的准则、模型学习的方法。  
**2.监督学习[^1]**  
监督学习的任务是学习一个模型，使模型能够对任意给定的输入，对其相应的输出做出一个好的预测。  
**假设空间**：  
由输入空间到输出空间映射的集合。  
**过拟合**：  
当模型的复杂度增大时，训练误差会减小，达到最小值后有增大。当选择的模型复杂度过大时，过拟合就会发生。  
**正则化**：  
结构风险最小化策略的实现，在经验风险上加上一个正则化项或法相，模型越复杂，正则化值越大。  
**交叉验证**:  
数据不充足时，为了更好地选择模型可采用交叉验证方法。  
**泛化误差:**  
采用最多的方法时通过测试误差评价学习方法的泛化能力，依赖于测试机。_用这个模型对未知数据预测的误差即为泛化误差_  
**生成模型与判别模型**  
__生成方法:__ _由数据学习联合概率分布，然后求出条件概率分布作为预测的模型（生成模型）_
___
### 感知机
__定义：__ 假设输入空间是X，输出空间是Y={+1， -1}，输入$x \in X$表示实例的特征向量，对应于输入空间的点；输出$y \in Y$表示实例的类别，由输入空间到输出空间的如下函数$$f(x)=sign(wx+b)$$称为感知机。  
__学习策略：__ 假设数据集是线性可分的，学习目标是球的一个能够将训练集正实例点和负实例点完全正确分开的分离超平面。
___  
__感知机学习算法：__  
*1.* 选取初始值$w_0, b_0$ 
*2.* 在训练集中选取初值$(x_i,y_i)$  
*3.* 如果$y_i(wx_i+b) \leq 0$  $$w \leftarrow w+ \eta y_ix_i$$  $$b \leftarrow b+ \eta y_i$$  
*4.* 转至2，直至训练集中无误分类点。  
__算法的收敛性__  
*1.* 存在满足条件的$||\vec{w_{opt}}|| =1$的超平面，$\vec{w_{opt}} \vec{x}_i = y_i(w_{opt}x_i + b_{opt})$ 将训练集完全正确分开；存在 $\gamma = min_i [ y_i(w_{opt}x_i + b_{opt})]$,对所有$i=1,2,...N$  
$$y_iy_i(w_{opt}x_i = y_i(w_{opt}x_i + b_{opt})$$  
*2.* 令$R=max||\vec{x_i}||$ ,则在训练数据集上的误分类次数k满足不等式$$k\leq [\frac {R}{\gamma}]^2$$   
__对偶形式__   
$$f(x) = sign[\displaystyle \sum^{N}_{j=1}{{ay_jx_j} \cdot x +b}]$$  
*1.* 选取初始值$a \leftarrow 0,b\leftarrow 0$ 
*2.* 在训练集中选取初值$(x_i,y_i)$  
*3.* 如果$$y_i [\displaystyle \sum^{N}_{j=1}{{ay_jx_j} \cdot x +b}]\leq 0$$  
$$a \leftarrow a_i+ \eta $$  $$b \leftarrow b+ \eta y_i$$   
*4.* 转至2，直至训练集中无误分类点。  
Gram矩阵$$G=[x_ix_j]_{N\times N}$$
![RUNOOB 图标](基础\机器学习\感知机对偶形式.png)

[^1]:supervised learning 

